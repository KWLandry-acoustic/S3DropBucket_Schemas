{
      "$schema": "https://raw.githubusercontent.com/KWLandry-acoustic/s3dropbucket_Schemas/main/json-schema-s3dropbucket_config.json?ref=1",

      "aws_region": "us-east-1",

      "xmlapiurl": "https://api-campaign-${config.region}-${config.pod}.goacoustic.com/XMLAPI",
      "restapiurl": "https://api-campaign-${config.region}-${config.pod}.goacoustic.com:443/rest",
      "authapiurl": "https://api-campaign-${config.region}-${config.pod}.goacoustic.com/oauth/token",
      //"connectapiurl": "https://app.goacoustic.com/api/graph",
      "connectapiurl": "https://connect-api-us-1.goacoustic.com/api/graph",

      //Send all Logging data to an S3 Bucket
      "s3dropbucket_logbucket": "s3dropbucket-logs",
      "s3dropbucket_log": "CloudWatch", //Log to the S3 bucket instead of CloudWatch (incomplete TBD.)

      //S3 Drop Bucket for this deployment of the Lambda function
      "s3dropbucket": "s3dropbucket",

      //S3 Work Processor Bucket for this deployment of the Lambda function
      "s3dropbucket_workbucket": "s3dropbucket-process",

      //Bulk Import Bucket for this deployment of the Lambda function
      "s3dropbucket_bulkimport": "ac-sftp-prod-us-east-1-upload",


      //S3 Work Processor Bucket for this deployment of the Lambda function
      "s3dropbucket_configs": "s3dropbucket-configs",

      //SQS Queue for this deployment of the Lambda function
      // "s3DropBucketWorkQueue": "https://sqs.us-east-1.amazonaws.com/777957353822/tricklercacheQueue",
      "s3dropbucket_workqueue": "https://sqs.us-east-1.amazonaws.com/777957353822/s3dropbucketQueue",

      //Firehose Stream definition for this deployment of s3dropbucket (case sensitive)
      "s3dropbucket_firehosestream": "S3DropBucket_Aggregator",

      //Quiesce Processing of the S3 Drop Bucket (Customer Facing Bucket for new Work) 
      "s3dropbucket_quiesce": false,

      //Quiesce Processing of the SQS Process/Work Queue, the Queue that manages all Update work
      "s3dropbucket_workqueuequiesce": false,

      //Quiesce Processing of the S3 Process/Work Bucket 
      // (Where all new work arriving in the Customer-facing Bucket is staged for updating Campaign) 
      "s3dropbucket_queuebucketquiesce": false,

      //Lookback hours for Orphaned incoming files. Any file with an Unmodified datetime attribute older than this 
      //    period will be sent to Processing/ReProcessing with the Unmodified datetime attribute updated. 
      //    See documentation for sending specific files back through processing/reprocessing.
      "s3dropbucket_mainthours": 0, //If set to 0 will not run a maintenance cycle
      //Limit the number of reprocessed objects to avoid "Please Slow Down" Errors
      "s3dropbucket_maintlimit": 10, // 0-10, If set to 1000 (max) this will indicate to process "Every" eligible 
      //    object (within the Hours age criteria) which could cause significant processing overhead and incur "Please Slowdown" errors
      "s3dropbucket_maintconcurrency": 1, //Number of Maintenance threads to run, Caution - as this is for evey single invocation higher than 10 can consume significant resources

      //Lookback hours for Orphaned Work. Any work file with an unmodified datetime attribute older than this 
      //    period will be sent back to the Work Queue for processing/reprocessing with the unmodified datetime attribute updated. 
      //    See documentation for sending specific work back through processing/reprocessing.
      "s3dropbucket_workqueuemainthours": 0, //If set to 0 will not run a maintenance cycle
      //Limit the number of reprocessed objects to avoid "Please Slow Down" Errors
      "s3dropbucket_workqueuemaintlimit": 10, //0-10, If set to 1000 (max) this will indicate to process "Every" eligible 
      //    object (within the Hours age criteria) which could cause significant processing overhead and incur "Please Slowdown" errors
      "s3dropbucket_workqueuemaintconcurrency": 1, //Number of Maintenance threads to run, Caution - as this is for evey single invocation higher than 10 can consume significant resources


      //ReProcess Work Queue Orphans - Useful in retrying work that fails due to a misconfigured Customer configuration. 
      //Specify a Customer prefix to requeue all work found on the tricklercache-process bucket 
      //reQueue = "customer_"            

      //Prefix Focus - Focus on processing only inbound DropBucket Objects with the prefix xxx (typically a Customer prefix)
      //Provided as an aid, See documentation for discussions on managing 
      // the S3 Buckets and SQS Queues of the DropBucket Solution.  
      "s3dropbucket_prefixfocus": "",

      // Check S3 documentation for EventEmitterMaxListeners Tuning 
      "s3dropbucket_eventemittermaxlisteners": 100,

      //JSON Parser Separator - Default needs to be "\n" but Customer config can define unique separator
      "s3dropbucket_jsonseparator": "\n",

      // Check SQS documentation for WorkQueueVisibilityTimeout Tuning 
      //Deprecated-Left for Visibilty into Queue Tuning
      // "WorkQueueVisibilityTimeout": 15, 

      //Check SQS Documentation for WorkQueueWaitTimeSeconds Tuning 
      //Deprecated-Left for Visibilty into Queue Tuning
      // "WorkQueueWaitTimeSeconds": 5,

      //Check SQS Documentation for RetryQueueVisibilityTimeout Tuning
      //Deprecated-Left for Visibilty into Queue Tuning
      // "RetryQueueVisibilityTimeout": 30,

      //Check SQS Documentation for RetryQueueInitialWaitTimeSeconds Tuning
      //Deprecated-Left for Visibilty into Queue Tuning
      // "RetryQueueInitialWaitTimeSeconds": 3,

      //Set a Warning threshold of the number of Batches parsed from a single Customer-facing Bucket file. 
      // New work arriving is parsed out in "Update Batches", if the number of batches exceeds this limit Logging 
      // will include a warning for Alerting on a possible issue with that S3 Object (file). 
      "s3dropbucket_maxbatcheswarning": 3000,


      //
      //   CAUTION:   
      //               Some of the following Selective Debug options create significant Logging volume and can 
      //               cause significant Logging Costs in AWS.
      //               Be Sure to remove as soon as possible after the Logging needed has been achieved.
      //          
      //   CAUTION:
      // 
      //Useful for Troubleshooting or monitoring, each Selective Debug turns on a detailed message about 
      //    various steps in the DropBucket Process.

      //Basic Messaging (001-100)
      //Message 001: Log
      //Message 002: Log
      //...
      //Message 97:   Log Environment Variables
      //Message 98:   Log Current S3DropBucket Options.
      //Message 99:   Log Current S3DropBucket Logging Options.
      //
      //S3DropBucket Inboard Processing Logging (300->499)
      //ToDo: move 9xx series messages to 300 series
      //
      //S3DropBucket ProcessBucket or Work Queue Logging (500->699) 
      //Message 500:  ProcessBucket - Log receiving a batch of Events to be processed.
      //Message 501:  ProcessBucket - Log data being read and processed for the S3 Object being processed.
      //Message 502:  ProcessBucket - Log opening of the file stream for the S3 Object being processed.
      //Message 503:  ProcessBucket - Log completion of processing all records of the S3 Object being processed.
      //Message 504:  ProcessBucket - Log Deletions of the S3 Object after successfully processed.  
      //Message 505:  ProcessBucket - Log Completion of processing all Events of this batch of processing for this Lambda invocation.
      //Message 506:  ProcessWorkQueue - Log receiving a batch of Work Queued Events to be processed for this Lambda invocation.
      //Message 507:  ProcessWorkQueue - Log each Work file Processing Start and Completion from the (SWS) Work Queue (with SQS Batch Fail Entries for Retry on completion) 
      //Message 508:  ProcessWorkQueue - Log Work Successfully (or Partially Successfully) Posted to Campaign from Queued Work. 
      //Message 509:  ProcessWorkQueue - Log SQSBatchFail Items
      //Message 510:  ProcessWorkQueue - Log completion of processing with the number of Queued Work records processed and number and list of Items to be Retried.
      //Message 511:  ProcessWorkQueue - Log short message of Updates POSTed
      //Message 512:  ProcessWorkQueue - Log the retrieval of S3 Objects off the Queued Work Bucket (Process bucket)
      //Message 513:  ProcessWorkQueue - Log the entire SQS Queue Message for each SQS Queue event. 
      //Message 514:  ProcessWorkQueue - Log the start of processing each S3 Object
      //Message 515:  ProcessWorkQueue - Log when an update exceeds MaxRows defined in Customers Config
      //Message 516:  ProcessWorkQueue - Log when work is set for Retry
      //Message 517:  ProcessWorkQueue - Log the pull of Work files from the Queued Work Bucket (Process bucket) 
      //Message 518:  ProcessWorkQueue - Log  
      //Message 519:  ProcessWorkQueue - Log  
      //Message 520:  ProcessWorkQueue - Log  
      //Message 521:  ProcessWorkQueue - Log  
      //Message 522:  ProcessWorkQueue - Log  
      //Message 523:  ProcessWorkQueue - Log  
      //Message 524:  ProcessWorkQueue - Log  
      //Message 525:  ProcessWorkQueue - Log BulkImport Write attempt
      //Message 526:  ProcessWorkQueue - Log when writing to Bulk Import is In-Quiesce
      //Message 527:  ProcessWorkQueue - Log Result of writing to BulkImport 
      //Message 528:  ProcessWorkQueue - Log  
      //Message 529:  ProcessWorkQueue - Log     
      //Message 530:  ProcessWorkQueue - Log  


      //...
      //S3DropBucket SFTP (700->799) 
      //Selective Debug 700: Log Event received 
      //Selective Debug 701: Log Selective Logging in effect
      //Selective Debug 702: Log  
      //Selective Debug 703: Log  
      //Selective Debug 704: Log  
      //Selective Debug 705: Log  
      //Selective Debug 706: Log
      //Selective Debug 707: Log
      //Selective Debug 708: Log  
      //Selective Debug 709: Log  
      //Selective Debug 710: Log  

      //S3DropBucket Connect (800->899) 
      //Selective Debug 800: Log
      //Selective Debug 801: Log
      //Selective Debug 802: Log
      //Selective Debug 803: Log
      //Selective Debug 804: Log
      //Selective Debug 805: Log

      //ToDo: Reorg 900 messaging into 300s and 500s 
      //Selective Debug 900:  Log AccessToken activity (refresh and use)
      //Selective Debug 901:  Log the current S3DropBucket Config 
      //Selective Debug 902:  Log the results of Parsing and Queueing an S3 Object. 
      //Selective Debug 903:  Log the end result of Processing each S3 Object.
      //Selective Debug 904:  Log the entire Batch of Queued Work Messages (SQS) received.  
      //Selective Debug 905:  Log the Updates about to be POSTed to Campaign
      //Selective Debug 906:  Log the JSON created for Campaign Relational Table Updates (also see 916 for DB, and 917 for XML)
      //Selective Debug 907:  Log the SQS Message being written to the SQS Queue and the write of the Work File (Batch of Updates) to the S3 Process Bucket 
      //Selective Debug 908:  Log the outcome of the Update POST to Campaign (also helpful to see events marked for Retry) Also see 510
      //Selective Debug 909:  Log Errors in the S3 Read Stream
      //Selective Debug 910:  Log Customer Config for each S3 Object about to be processed. 
      //Selective Debug 911:  Log each SQS Event of a Batch of Events being processed (also see 904)
      //Selective Debug 912:  Log the retrieval of the S3DropBucket Configuration file (useful for deployment in new regions)
      //Selective Debug 913:  Log each OnData record (chunk) parsed in the Read Stream of Processing an S3 Object.
      //Selective Debug 914:  Log the Work file and outcome of Adding Work to the SQS Queue - Queueing the Work
      //Selective Debug 915:  Log the outcome of Storing and Queing Updates to the Work Bucket and Work Queue respectively. 
      //Selective Debug 916:  Log the JSON created for Campaign Database Updates (also see 906 for RT)
      //Selective Debug 917:  Log the XML created for Campaign Updates from the JSON (see also 906)
      //Selective Debug 918:  Log the Object processed, number of updates it contained and the current Batch count about to be packaged as Queued Work. 
      //Selective Debug 919:  Log the Transforms configured and applied.
      //Selective Debug 920:  Log the final outcome of all S3DropBucket processing 
      //Selective Debug 921:  Log the final outcome of all Queued Work (create SQS Queue Event and Queued Work file (Process Bucket)) processing
      //Selective Debug 922:  Log detailed result of sending *each* update to Firehose Aggregator (see 942 for summary reporting as this can be hundreds/thousands of updates reported)
      //Selective Debug 923:  Log any Quiesce in force
      //Selective Debug 924:  Log each Delete of a successful processing of a Queued Work file
      //Selective Debug 925:  Log the Processing of an Aggregation file (Firehose Aggregator).
      //Selective Debug 926:  Log POST Update Success
      //Selective Debug 927:  Log POST Update Failures 
      //Selective Debug 928:  Log Partial POST Update Success
      //Selective Debug 929:  Log Temporary POST Update Fails
      //Selective Debug 930:  Log Transform-JSONPath messaging 
      //Selective Debug 931:  Log Transform-CSV messaging
      //Selective Debug 932:  Log Transform-Ignore messaging
      //Selective Debug 933:  Log Transform Errors (currently only daydate)
      //Selective Debug 934:  Log Transform Errors/Exceptions 
      //Selective Debug 935:  Log Hard Failures when POSTing updates
      //Selective Debug 936:  Log the result of each POST regardless of outcome
      //Selective Debug 937:  Log Prefix Focus in effect (S3DropBucket Config Filter to ignre any other work except that with this prefix)
      //Selective Debug 937:  Log 
      //Selective Debug 937:  Log 
      //Selective Debug 938:  Log 
      //Selective Debug 939:  Log 
      //Selective Debug 940:  Log 
      //Selective Debug 941:  Log each set of updates sent through PackageUpdates-StoreAndQueueWork (caution: can be voluminous data). Useful for Onboarding mapping.
      //Selective Debug 942:  Log result/summary of sending inbound updates to Firehose Aggregator (see 922 for detailed logging of each update)
      //Selective Debug 943:  Log completion of file processing to FireHose Aggregator
      //Selective Debug 944:  Log 
      //Selective Debug 945:  Log 
      //Selective Debug 946:  Log                                          
      //...
      //...
      //Selective Debug 999: 


      // Selective MUST have a preceding underscore and trailing comma, as shown: 
      "s3dropbucket_selectivelogging": "_503,_504,_509,_511,_826,_908,_918,_924,_925,_927,_928,_929,_930,_931,_932,_933,_934,_942,_943,",
      "s3dropbucket_loglevel": "NORMAL",
      //"loglevel": "ALL",
      // ALL - Log every message generated regardless of Selective Logging Config
      // NONE - Log Nothing (Exceptions will continue to log)
      // NORMAL - Follow the logging defined in SelectiveLogging 

      //TBD: Deprecate in favor of using AWS UI or CLI 
      //Useful for cleaning out the S3 DropBucket. 
      //DropBucketPurgeCount sets the number to purge at one time (Max 1000) 
      //DropBucketPurge provides the means to name a custom-version of the DropBucket 
      //Provided as an aid only, 
      // Preferred - See documentation for examples using AWS CLI, a preferred method. 
      "s3dropbucket_purgecount": 0,
      "s3dropbucket_purge": "S3DropBucket",

      //TBD: Deprecate in favor of using AWS UI or CLI 
      //Useful for cleaning out the  TricklerCache Work Queue Bucket. 
      //WorkQueueBucketPurgeCount sets the number to purge at one time (Max 1000) 
      //WorkQueueBucketPurge provides the means to name a custom-version of the DropBucket Work Queue Bucket
      //Provided as an aid only, 
      //Preferred - See documentation for examples using AWS CLI, a preferred method. 
      "s3dropbucket_workqueuebucketpurgecount": 0,
      "s3dropbucket_workqueuebucketpurge": "S3DropBucket-process"

}


